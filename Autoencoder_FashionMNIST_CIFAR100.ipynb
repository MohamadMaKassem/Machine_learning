{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohammadMahdi1128/Machine_learning/blob/main/Autoencoder_FashionMNIST_CIFAR100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MohammadMahdi1128/Machine_learning.git\n",
        "%cd /content/Machine_learning"
      ],
      "metadata": {
        "id": "YpJfNb_V8r62",
        "outputId": "e18a765a-6886-419b-fdac-a33e8c184513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YpJfNb_V8r62",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine_learning'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 25 (delta 13), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (25/25), 1.04 MiB | 10.90 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "/content/Machine_learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f551add",
      "metadata": {
        "id": "3f551add"
      },
      "source": [
        "# Autoencoder for Fashion MNIST and CIFAR-100\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "In this project, you will implement an Autoencoder to explore and generate hybrid images by combining feature vectors from different classes. This notebook focuses on using the **Fashion MNIST** dataset, and as a challenge, you will later apply the same methodology to the **CIFAR-100** dataset.\n",
        "\n",
        "**Tasks:**\n",
        "1. Dataset Preparation and Filtering\n",
        "2. Autoencoder Implementation\n",
        "3. Training the Autoencoder\n",
        "4. Class Feature Centroid Calculation\n",
        "5. Average Image Creation\n",
        "6. Hybrid Image Generation\n",
        "7. CIFAR-100 Challenge Exercise\n",
        "\n",
        "Final Goal. Create hybrid objects. E.g. First a hybrid between a sneaker and a t-shirt and later a hybrid between a car and a plane.\n",
        "\n",
        "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
        "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
        "- Steps taken to alleviate difficulties\n",
        "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
        "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
        "- If you have an idea how this could be extended in an interesting way, describe it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6a3505",
      "metadata": {
        "id": "ef6a3505"
      },
      "source": [
        "\n",
        "## Step 1: Dataset Preparation\n",
        "\n",
        "We will work with the **Fashion MNIST** dataset, which contains 10 classes of grayscale images representing items of clothing.\n",
        "\n",
        "### Your Tasks:\n",
        "1. Load the Fashion MNIST dataset using `torchvision.datasets`.\n",
        "2. Apply necessary transformations, including:\n",
        "   - Normalization to scale pixel values.\n",
        "   - Resizing if needed.\n",
        "3. Create training and validation DataLoaders for efficient data loading.\n",
        "\n",
        "### Hints:\n",
        "- Use `torchvision.transforms` for preprocessing.\n",
        "- Normalize images to have mean `0.5` and standard deviation `0.5`.\n",
        "\n",
        "Start by writing your code to load and preprocess the dataset.\n",
        "\n",
        "## **Please note, you can use code from https://github.com/junaidaliop/pytorch-fashionMNIST-tutorial/blob/main/pytorch_fashion_mnist_tutorial.ipynb to make it easier to load the dataset. However, whenever you copy&paste code without modifications you need to write a comment where you copied that code from.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e095a27",
      "metadata": {
        "id": "3e095a27"
      },
      "outputs": [],
      "source": [
        "# Import the necessary transformations module from torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define a transformation pipeline.\n",
        "# Here, we're only converting the images to PyTorch tensor format.\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Using torchvision, load the Fashion MNIST training dataset.\n",
        "# root specifies the directory where the dataset will be stored.\n",
        "# train=True indicates that we want the training dataset.\n",
        "# download=True will download the dataset if it's not present in the specified root directory.\n",
        "# transform applies the defined transformations to the images.\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create a data loader for the training set.\n",
        "# It will provide batches of data, in this case, batches of size 4.\n",
        "# shuffle=True ensures that the data is shuffled at the start of each epoch.\n",
        "# num_workers=2 indicates that two subprocesses will be used for data loading.\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Similarly, load the Fashion MNIST test dataset.\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define the class labels for the Fashion MNIST dataset.\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ce7c90-9d5b-4b23-83b4-21dc0be120ef",
      "metadata": {
        "id": "26ce7c90-9d5b-4b23-83b4-21dc0be120ef"
      },
      "source": [
        "## Step 2: Autoencoder Implementation\n",
        "\n",
        "A rudimentary implementation of an Autoencoder is given here. You may need to modify it depending on your needs. That can mean adding more convolutional layers.\n",
        "\n",
        "Note that you need to change the kernel size and stride potentially.\n",
        "\n",
        "Depending on your input size the Adaptive Pooling may be used on a very large feature map which can reduce the performance. You will need to figure out the sizes of the input as you add more convolutional layers. One way is to remove outputs from the encoder, for example the AdaptiveAvgPool2d and then print the output shape of the encoder. After you can stop execution for example with \"assert False\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94aee3f0-6a17-4c46-9ba4-2811ab5bf61b",
      "metadata": {
        "id": "94aee3f0-6a17-4c46-9ba4-2811ab5bf61b"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Cell 2: Define the Autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_channels=1, latent_dim=128):\n",
        "        \"\"\"\n",
        "        Autoencoder with dynamic adjustments for varying input image dimensions.\n",
        "\n",
        "        Parameters:\n",
        "        - input_channels: Number of input channels (e.g., 1 for grayscale, 3 for RGB).\n",
        "        - latent_dim: Dimensionality of the latent representation.\n",
        "        \"\"\"\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder: Dynamically adapts to reduce input to a fixed latent space\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),  # Halve dimensions\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # Halve dimensions again\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # Further downsample\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))  # Compress to a fixed-size latent space (1x1 feature map)\n",
        "        )\n",
        "\n",
        "        # Latent space representation\n",
        "        self.latent = nn.Linear(128, latent_dim)  # Flatten into 1D latent space\n",
        "\n",
        "        # Decoder: Dynamically expands the latent representation back to the input shape\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),  # Expand back to initial channel size\n",
        "            nn.Unflatten(1, (128, 1, 1)),  # Reshape to (C, H, W) for convolutional operations\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Double dimensions\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # Double dimensions again\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),  # Final upsample\n",
        "            nn.Sigmoid()  # Ensure output values are between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the Autoencoder.\n",
        "\n",
        "        - Encodes the input into a fixed-size latent representation.\n",
        "        - Decodes the latent representation back to the input shape.\n",
        "        \"\"\"\n",
        "        # Encode\n",
        "        encoded = self.encoder(x)\n",
        "        encoded = encoded.view(encoded.size(0), -1)  # Flatten to pass through linear layer\n",
        "        latent = self.latent(encoded)  # Map to latent space\n",
        "\n",
        "        # Decode\n",
        "        decoded = self.decoder(latent)\n",
        "        return decoded\n",
        "\n",
        "# Cell 3: Initialize the Model\n",
        "# Define hyperparameters for the model\n",
        "input_channels = 1  # Example: Grayscale images\n",
        "latent_dim = 128  # Latent space dimensionality\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the Autoencoder\n",
        "model = Autoencoder(input_channels=input_channels, latent_dim=latent_dim).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294fcef1-c01f-4590-8f4f-757a2bcf257e",
      "metadata": {
        "id": "294fcef1-c01f-4590-8f4f-757a2bcf257e"
      },
      "source": [
        "### Your Tasks:\n",
        "1. Implement the encoder and decoder parts of the Autoencoder.\n",
        "2. Ensure the model takes grayscale images (1 input channel) and outputs images of the same shape.\n",
        "\n",
        "### Hints:\n",
        "- Use `nn.Conv2d` and `nn.ConvTranspose2d`.\n",
        "- Add non-linear activations like `ReLU` between layers.\n",
        "- Use `Tanh` or `Sigmoid` for the final activation in the decoder.\n",
        "\n",
        "Write your Autoencoder model below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b4b363",
      "metadata": {
        "id": "b3b4b363"
      },
      "source": [
        "\n",
        "## Step 3: Training the Autoencoder\n",
        "\n",
        "Train your Autoencoder to reconstruct images from the Fashion MNIST dataset.\n",
        "\n",
        "### Your Tasks:\n",
        "1. Define a suitable loss function (e.g., Mean Squared Error).\n",
        "2. Set up an optimizer like Adam.\n",
        "3. Write a training loop to:\n",
        "   - Pass inputs through the Autoencoder.\n",
        "   - Compute the reconstruction loss.\n",
        "   - Backpropagate and update weights.\n",
        "\n",
        "4. Visualize the reconstructed images periodically during training.\n",
        "\n",
        "### Hints:\n",
        "- Use GPU acceleration if available (`.cuda()`).\n",
        "- Visualize outputs using `matplotlib`.\n",
        "\n",
        "Write your training loop below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f598d7",
      "metadata": {
        "id": "f4f598d7"
      },
      "source": [
        "\n",
        "## Step 4: Latent Space Analysis\n",
        "\n",
        "Once the Autoencoder is trained, explore the latent space.\n",
        "\n",
        "### Your Tasks:\n",
        "1. Pass a batch of images through the encoder and store the latent representations.\n",
        "2. Compute the **centroids** (average latent vectors) for each Fashion MNIST class.\n",
        "3. Visualize the latent space using dimensionality reduction techniques like PCA or t-SNE.\n",
        "\n",
        "### Hints:\n",
        "- Use `sklearn.decomposition.PCA` or `sklearn.manifold.TSNE` for visualization.\n",
        "- Compute centroids by averaging latent vectors of images from the same class.\n",
        "\n",
        "Write your code to analyze the latent space below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958d4d04",
      "metadata": {
        "id": "958d4d04"
      },
      "source": [
        "\n",
        "## Step 5: Hybrid Image Generation\n",
        "\n",
        "Using the latent space centroids, you can create hybrid images by interpolating between centroids of two classes.\n",
        "\n",
        "### Your Tasks:\n",
        "1. Select two class centroids (e.g., \"T-shirt\" and \"Sneaker\").\n",
        "2. Linearly interpolate between the centroids with a parameter `alpha` in [0, 1].\n",
        "3. Decode the interpolated latent representations back into image space.\n",
        "\n",
        "### Hints:\n",
        "- Use a simple linear interpolation formula: `(1 - alpha) * centroid1 + alpha * centroid2`.\n",
        "- Visualize the hybrid images for different values of `alpha`.\n",
        "\n",
        "Write your code to generate hybrid images below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5ec5c5",
      "metadata": {
        "id": "0f5ec5c5"
      },
      "source": [
        "\n",
        "## Step 7: Challenge Exercise: Reimplement with CIFAR-100\n",
        "\n",
        "Now that you've successfully implemented the Autoencoder for Fashion MNIST, your next challenge is to apply the same pipeline to the **CIFAR-100 dataset**. This dataset contains 100 classes of images, each with diverse objects, making it more challenging than Fashion MNIST.\n",
        "\n",
        "### Your Tasks:\n",
        "1. Preprocess the CIFAR-100 dataset, ensuring the images are appropriately normalized and resized if needed.\n",
        "2. Redefine the Autoencoder architecture to accommodate CIFAR-100's RGB images (3 channels).\n",
        "3. Train the Autoencoder on CIFAR-100 and visualize the reconstructed images.\n",
        "4. Compute class centroids in the latent space for selected CIFAR-100 classes (choose a manageable subset, such as 10 classes).\n",
        "5. Generate hybrid images by interpolating between class centroids in the latent space.\n",
        "6. Visualize the latent space clustering for the selected CIFAR-100 classes using PCA or t-SNE.\n",
        "\n",
        "This exercise will test your understanding of the Autoencoder pipeline and challenge you to work with a more complex dataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base]",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}